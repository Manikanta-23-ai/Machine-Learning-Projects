# Machine-Learning-Projects
Developed ML projects including Iris flower classification, email spam detection, and house price prediction. Worked on data preprocessing, feature engineering, and applying algorithms like SVM, Naive Bayes, Linear Regression, and Random Forest with strong model evaluation and accuracy.
1. Iris Flower Classification – Supervised Machine Learning (Classification)

This project focuses on predicting the species of an iris flower—Setosa, Versicolor, or Virginica—using sepal and petal measurements.
The workflow includes data loading, exploratory data analysis, visualizations, and preprocessing. Multiple classification algorithms such as K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Logistic Regression, and Decision Trees were trained.
Model performance was evaluated using accuracy, confusion matrix, and cross-validation. The final model achieved high accuracy.

Tools & Libraries: Python, Jupyter Notebook, Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn

2. Email Spam Detection – Natural Language Processing (Text Classification)

This project builds a machine learning model to classify emails as spam or ham.
Raw email text is cleaned through preprocessing steps including lowercasing, stopword removal, punctuation removal, stemming/lemmatization, and tokenization. Text is then converted into numerical features using TF-IDF vectorization.
Machine learning models such as Naive Bayes, Logistic Regression, and Support Vector Machine (SVM) were trained. Performance metrics like precision, recall, F1-score, and confusion matrix were used to evaluate results.

Tools & Libraries: Python, Pandas, NumPy, NLTK, Scikit-learn, Jupyter Notebook

3. House Price Prediction – Supervised Machine Learning (Regression)

This project aims to predict real estate prices based on features such as square footage, number of rooms, location, and property age.
Data preprocessing included handling missing values, outlier detection, encoding categorical variables, and feature scaling.
Regression models such as Linear Regression, Random Forest Regressor, Gradient Boosting Regressor, and XGBoost were trained and evaluated. Model performance was measured using RMSE, MAE, and R² Score, and feature importance analysis identified key price drivers.

Tools & Libraries: Python, Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, XGBoost, Jupyter Notebook
